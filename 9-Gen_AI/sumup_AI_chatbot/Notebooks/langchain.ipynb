{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Development of a Sumup Ai Chatbot**  \n",
    "**===== Langchain setup =====**\n",
    "\n",
    "---\n",
    "\n",
    "By Jean-Yves Tran | jy.tran@[datascience-jy.com](https://datascience-jy.com) | [LinkedIn](https://www.linkedin.com/in/jytran-datascience/)  \n",
    "IBM Certified Data Analyst \n",
    "\n",
    "---\n",
    "\n",
    "Source: \n",
    "- [OpenAI documentation](https://platform.openai.com/docs/overview)\n",
    "- [Langchain on Python](https://python.langchain.com/docs/introduction/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive links in this notebook are not working due to GitHub limitations. View this notebook with the interactive links working [here](https://nbviewer.org/github/jendives2000/Data_ML_Practice_2025/blob/main/1-3-SQL/practice/DuckDB/notebooks/1_duckdb_intro.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain_openai\n",
    "# this will install the latest version of Langchain\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part of the ML Pro bootcamp I started in September 2024. \n",
    "\n",
    "I develop this chatbot with the OpenAI API, on langchain with Python (3.12.8). \n",
    "\n",
    "The langchain version the **v0.3**. The model used is the **3.5-turbo**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Langchain**: \n",
    "\n",
    "This framework manages for you all the API endpoints of the LLMs you need in your app project.  \n",
    "\n",
    "While it is easy to understand the way it works, some key knowledge will help doing it the right way: \n",
    "- Which version of Langchain? \n",
    "  - Versions change each bring significant changes, impacting the way Langchain is used. The version you imported may not be the one that is familiar to the rest of the team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=os.environ[\"MODEL_ID\"],\n",
    "    temperature = 0.17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I can call the invoke method to say something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    \"Hey, how are you doing?\"\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why use Langchain if it is for such a basic use? \n",
    "What I want now is, for example, to:\n",
    "- automate a translation to French\n",
    "- on everything I write to it\n",
    "\n",
    "For that I need a prompt object, which is just my text prompt assigned to a variable and inside it a reference to the original message (or rather prompt). The module `PromptTemplate` that I already imported is also needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Translate to French: {original_message}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to instantiate a PromptTemplate object and pass that `prompt_template` I just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"original_message\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, I need to define the chain to apply to my objects and invoke my prompt using it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Salut là! Ne seriez-vous pas le shérif par hasard?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke(\"Howdy there! Wouldn't you happen to be the Sheriff?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
