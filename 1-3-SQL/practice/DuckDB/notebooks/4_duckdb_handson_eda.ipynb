{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4: Hands-on Exploratory Data Analysis with DuckDB :duck:**\n",
    "\n",
    "---\n",
    "\n",
    "By Jean-Yves Tran | jy.tran@[datascience-jy.com](https://datascience-jy.com) | [LinkedIn](https://www.linkedin.com/in/jytran-datascience/)  \n",
    "IBM Certified Data Analyst \n",
    "\n",
    "---\n",
    "\n",
    "Source: \n",
    "- [Getting Started with DuckDB](https://www.packtpub.com/en-ar/product/getting-started-with-duckdb-9781803232539) by Simon Aubury & Ned Letcher\n",
    "- [DuckDB documentation](https://duckdb.org/docs/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive links in this notebook are not working due to GitHub limitations. View this notebook with the interactive links working [here](https://nbviewer.org/github/jendives2000/Data_ML_Practice_2025/blob/main/1-3-SQL/practice/DuckDB/notebooks/4_duckdb_handson_eda.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part 3 of this series of notebooks on DuckDB.  \n",
    "For an introduction to DuckDB, check [my first notebook](https://github.com/jendives2000/Data_ML_Practice_2025/blob/82571ad44176666f9cf0735c5141c6a96d5eace9/1-3-SQL/practice/DuckDB/notebooks/1_duckdb_intro.ipynb). I also say in there when you should not use DuckDB. \n",
    "For understanding how DuckDB works, check my [second notebook](https://github.com/jendives2000/Data_ML_Practice_2025/blob/ef8533ad82586234cfdc54a494c0c5be590816cc/1-3-SQL/practice/DuckDB/notebooks/2_duckdb_python_API.ipynb).\n",
    "\n",
    "Here I will learn about best practices that:\n",
    "- save time when:\n",
    "  - [querying](#select-) or [inserting](#insert) data to a DuckDB database\n",
    "  - joining tables ([positional](#positional-joins) and [temporal joins](#temporal-joins-asof))\n",
    "\n",
    "<u>**SqlMagic:**</u>  \n",
    "\n",
    "You will notice starting from the insert chapter that I refactored a code that runs SQL commands from the DuckDB shell (CLI). I named it shell_commd(). \n",
    "There is a [better setup](https://duckdb.org/docs/guides/python/jupyter.html) for jupyter-based works that essentially:\n",
    "- runs SQL cells in Jupyter (using just '%sql' before any query) \n",
    "- and delivers the output as a pandas dataframe. \n",
    "\n",
    "Such a setup requires to install and import jupysql, pandas of course and optionally matplotlib and duck-engine.  \n",
    "I am **not using this setup** in this notebook but in the next one, which will be focused on data exploration and visualization. \n",
    "\n",
    "**OUTLINE:**  \n",
    "In more details, I will cover the followings:\n",
    "- [**Selecting columns**](#select-) effectively\n",
    "- Applying [**function chaining**](#function-chaining)\n",
    "- Using [**INSERT**](#insert) effectively\n",
    "- Leveraging [**positional** joins](#positional-joins) and [**temporal joins**](#temporal-joins-asof)\n",
    "- [**Recursive** queries](#hierarchical-traversal) and [macros](#macros)\n",
    "- additional **tips and tricks**\n",
    "\n",
    "**NICETIES:**  \n",
    "Some of the nicest little improvements brought by DuckDB are: \n",
    "- [trailing comma](#trailing-comma-is-fine) not a problem\n",
    "- the [exclude](#exclude-columns) clause\n",
    "- [visual bars](#visualize-bars-in-the-output) in the output\n",
    "- [insert or replace](#insert-or-replace)\n",
    "\n",
    "**SKIERS DATABASE**:  \n",
    "I added several very simple Skiers Database and csv files in the data/data_in folder: `skiers.csv`  \n",
    "They'll be used throughout this notebook. \n",
    "\n",
    "**The main takeaway is**:\n",
    "- to **better comprehend** the **differences** between regular SQL queries and DuckDB enhanced queries, which is often a lot less verbosity and more readability. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas matplotlib\n",
    "import duckdb\n",
    "import jupysql_plugin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading JupySQL Magic for later use:\n",
    "#%load_ext sql\n",
    "#%config SqlMagic.autopandas = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting back to the persistent disk-based database used in the EDA chapter\n",
    "#%sql duckdb:///../data/data_out/pedestrian.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset: Melbourne Pedestrian Count**:\n",
    "\n",
    "I will be using a dataset made available by the city of Melbourne, Australia. contains hourly pedestrian counts from pedestrian sensors located in and around the Melbourne Central business district. We’ll be working with a historical timeframe of this dataset ranging from 2009 to 2022.\n",
    "\n",
    "I [imported](https://data.melbourne.vic.gov.au/api/datasets/1.0/pedestrian-counting-system-monthly-counts-per-hour/attachments/pedestrian_counting_system_monthly_counts_per_hour_may_2009_to_14_dec_2022_csv_zip/) it in the data/data_in folder. \n",
    "Once unzipped, this is:\n",
    "- a 420MB dataset \n",
    "- with over 2,1 million entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Relational API**: \n",
    "\n",
    "I will use the Relational API because it offers more for what I will be doing: data exploratory analysis. \n",
    "\n",
    "Let's get our dataset into a Relational Object (RO from now on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬───────────────────────────────┬───────┬──────────┬───────┬──────────┬───────┬───────────┬───────────────────────────────┬───────────────┐\n",
      "│   ID    │           Date_Time           │ Year  │  Month   │ Mdate │   Day    │ Time  │ Sensor_ID │          Sensor_Name          │ Hourly_Counts │\n",
      "│  int64  │            varchar            │ int64 │ varchar  │ int64 │ varchar  │ int64 │   int64   │            varchar            │     int64     │\n",
      "├─────────┼───────────────────────────────┼───────┼──────────┼───────┼──────────┼───────┼───────────┼───────────────────────────────┼───────────────┤\n",
      "│ 2887628 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        34 │ Flinders St-Spark La          │           300 │\n",
      "│ 2887629 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        39 │ Alfred Place                  │           604 │\n",
      "│ 2887630 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        37 │ Lygon St (East)               │           216 │\n",
      "│ 2887631 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        40 │ Lonsdale St-Spring St (West)  │           627 │\n",
      "│ 2887632 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        36 │ Queen St (West)               │           774 │\n",
      "│ 2887633 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        29 │ St Kilda Rd-Alexandra Gardens │           644 │\n",
      "│ 2887634 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        42 │ Grattan St-Swanston St (West) │           453 │\n",
      "│ 2887635 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        43 │ Monash Rd-Swanston St (West)  │           387 │\n",
      "│ 2887636 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        44 │ Tin Alley-Swanston St (West)  │            27 │\n",
      "│ 2887637 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        35 │ Southbank                     │          2691 │\n",
      "│    ·    │               ·               │    ·  │    ·     │     · │   ·      │     · │         · │     ·                         │            ·  │\n",
      "│    ·    │               ·               │    ·  │    ·     │     · │   ·      │     · │         · │     ·                         │            ·  │\n",
      "│    ·    │               ·               │    ·  │    ·     │     · │   ·      │     · │         · │     ·                         │            ·  │\n",
      "│ 2897597 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        27 │ QV Market-Peel St             │           371 │\n",
      "│ 2897598 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        28 │ The Arts Centre               │          1188 │\n",
      "│ 2897599 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        31 │ Lygon St (West)               │           229 │\n",
      "│ 2897600 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        30 │ Lonsdale St (South)           │           391 │\n",
      "│ 2897601 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        34 │ Flinders St-Spark La          │           111 │\n",
      "│ 2897602 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        37 │ Lygon St (East)               │           133 │\n",
      "│ 2897603 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        40 │ Lonsdale St-Spring St (West)  │           154 │\n",
      "│ 2897604 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        36 │ Queen St (West)               │           249 │\n",
      "│ 2897605 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        29 │ St Kilda Rd-Alexandra Gardens │           448 │\n",
      "│ 2897606 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        42 │ Grattan St-Swanston St (West) │           288 │\n",
      "├─────────┴───────────────────────────────┴───────┴──────────┴───────┴──────────┴───────┴───────────┴───────────────────────────────┴───────────────┤\n",
      "│ ? rows (>9999 rows, 20 shown)                                                                                                          10 columns │\n",
      "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = duckdb.read_csv(\"../data/data_in/pedestrian_records_2009-2022.csv\")\n",
    "\n",
    "# 200 is the max number of characters possible inside an entry:\n",
    "records.show(max_width = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looking at the Data**:\n",
    "\n",
    "Because this is an RO, DuckDB loaded 10,000 rows and this what it **lazily returned** us here. There more entries than these 10,000. \n",
    "\n",
    "Lots of info in this dataset: \n",
    "- **count of pedestrians** detected by \n",
    "- a specific **sensor** \n",
    "- during **each hour**. \n",
    "- Additionally, it provides other details related to the hourly readings, including the **sensor name** \n",
    "  - and the **timestamp**, along with date and time components derived from the timestamp.\n",
    "\n",
    "### **Data types**:\n",
    "Notice in the header of the output that datetime is of the data type VARCHAR, meaning text. It should be a timestamp (docs on this [here](http://duckdb.org/docs/sql/functions/dateformat)). Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = duckdb.read_csv(\n",
    "    \"../data/data_in/pedestrian_records_2009-2022.csv\",\n",
    "    dtype={\"Date_Time\": \"TIMESTAMP\"},\n",
    "    timestamp_format=\"%B %d, %Y %H:%M:%S %p\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm this change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬─────────────────────┬───────┬──────────┬───────┬─────────┬───────┬───────────┬──────────────────────────────┬───────────────┐\n",
      "│   ID    │      Date_Time      │ Year  │  Month   │ Mdate │   Day   │ Time  │ Sensor_ID │         Sensor_Name          │ Hourly_Counts │\n",
      "│  int64  │      timestamp      │ int64 │ varchar  │ int64 │ varchar │ int64 │   int64   │           varchar            │     int64     │\n",
      "├─────────┼─────────────────────┼───────┼──────────┼───────┼─────────┼───────┼───────────┼──────────────────────────────┼───────────────┤\n",
      "│ 2887628 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        34 │ Flinders St-Spark La         │           300 │\n",
      "│ 2887629 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        39 │ Alfred Place                 │           604 │\n",
      "│ 2887630 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        37 │ Lygon St (East)              │           216 │\n",
      "│ 2887631 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        40 │ Lonsdale St-Spring St (West) │           627 │\n",
      "│ 2887632 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        36 │ Queen St (West)              │           774 │\n",
      "└─────────┴─────────────────────┴───────┴──────────┴───────┴─────────┴───────┴───────────┴──────────────────────────────┴───────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records.limit(5).show(max_width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enums for low cardinality String Columns**:\n",
    "\n",
    "[ENUM types](https://duckdb.org/docs/sql/data_types/enum.html) are a way to **convert string values into numbers** in a database. This is useful for columns with a limited number of different values, like month names and days of the week. By using ENUMs for these columns, we can **save storage space** and **speed up queries** because the database only stores numbers instead of full strings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DuckDB, because the **SQL parser doesn’t support subqueries** in a CREATE TYPE ... AS ENUM statement, the best approach is to **extract** the distinct values via SQL (or pandas), **format** them into a comma-separated list, and then **build** the ENUM type dynamically.  \n",
    "This method gives me flexibility and control over the ENUM values while keeping my code concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactored logic to create ENUM with an RO:\n",
    "def create_enum(col, db, enum_name):\n",
    "    \"\"\"\n",
    "    Create an ENUM type in DuckDB from unique values in a specified column.\n",
    "\n",
    "    Parameters:\n",
    "    col (str): The column name to extract unique values from.\n",
    "    db (str): The name of the DuckDB relation (table).\n",
    "    enum_name (str): The name of the ENUM type to be created.\n",
    "    \"\"\"\n",
    "    # extracting the unique values from the specified column\n",
    "    col_forEnum = duckdb.sql(\n",
    "        f\"\"\"\n",
    "        select distinct {col}\n",
    "        from {db}\n",
    "        \"\"\"\n",
    "    ).fetchall()\n",
    "\n",
    "    # Build a comma-separated list of ENUM values\n",
    "    col_enum_val = \", \".join(f\"'{v[0]}'\" for v in col_forEnum)\n",
    "\n",
    "    # create the ENUM type\n",
    "    duckdb.sql(f\"create type {enum_name} as enum ({col_enum_val});\")\n",
    "\n",
    "create_enum(\"Month\", \"records\", \"month_enum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enum(\"Day\", \"records\", \"day_enum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did I actually created them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                enum_range(CAST(NULL AS month_enum))                                │\n",
      "│                                             varchar[]                                              │\n",
      "├────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [March, December, July, January, October, February, May, June, November, April, August, September] │\n",
      "└────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────┐\n",
      "│                enum_range(CAST(NULL AS day_enum))                │\n",
      "│                            varchar[]                             │\n",
      "├──────────────────────────────────────────────────────────────────┤\n",
      "│ [Wednesday, Monday, Thursday, Friday, Saturday, Tuesday, Sunday] │\n",
      "└──────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(duckdb.sql(\"select enum_range(NULL::month_enum);\"))\n",
    "print(duckdb.sql(\"select enum_range(NULL::day_enum);\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selecting useful Columns**:\n",
    "\n",
    "Before we load our dataset into an on-disk database for analysis, we should think about any data changes we might want to make. \n",
    "\n",
    "For instance, we can remove columns we won’t need.   \n",
    "The additional date and time fields are likely to be useful, and we should keep the Sensor_ID column since it helps connect this dataset with another one from the city of Melbourne that includes details about each sensor, like their locations.  \n",
    "However, we can **drop the ID field** because it doesn’t relate to any other datasets about the pedestrian counting system.  \n",
    "Since we’ll be doing various time series analyses, we also need to **sort the records by the Date_Time column**.  \n",
    "\n",
    "Let’s make these changes and take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_v2 = records.select(\"* exclude ID\").sort(\"Date_Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────┬───────┬─────────┬───────┬─────────┬───────┬───────────┬───────────────────────────────────┬───────────────┐\n",
      "│      Date_Time      │ Year  │  Month  │ Mdate │   Day   │ Time  │ Sensor_ID │            Sensor_Name            │ Hourly_Counts │\n",
      "│      timestamp      │ int64 │ varchar │ int64 │ varchar │ int64 │   int64   │              varchar              │     int64     │\n",
      "├─────────────────────┼───────┼─────────┼───────┼─────────┼───────┼───────────┼───────────────────────────────────┼───────────────┤\n",
      "│ 2009-05-01 00:00:00 │  2009 │ May     │     1 │ Friday  │     0 │         4 │ Town Hall (West)                  │           209 │\n",
      "│ 2009-05-01 00:00:00 │  2009 │ May     │     1 │ Friday  │     0 │         1 │ Bourke Street Mall (North)        │            53 │\n",
      "│ 2009-05-01 00:00:00 │  2009 │ May     │     1 │ Friday  │     0 │         6 │ Flinders Street Station Underpass │           139 │\n",
      "│ 2009-05-01 00:00:00 │  2009 │ May     │     1 │ Friday  │     0 │         5 │ Princes Bridge                    │           157 │\n",
      "│ 2009-05-01 00:00:00 │  2009 │ May     │     1 │ Friday  │     0 │         2 │ Bourke Street Mall (South)        │            52 │\n",
      "└─────────────────────┴───────┴─────────┴───────┴─────────┴───────┴───────────┴───────────────────────────────────┴───────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records_v2.limit(5).show(max_width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this pre-analysis work is done, the data is good to go.  \n",
    "\n",
    "## **Advantages of loading into a disk-based database**:\n",
    "I need to load it into a persistent disk-based database so the whole data is written and saved, making it available for another time or another person. \n",
    "\n",
    "This means I will NOT use the default database duckdb anymore. The data I cleaned up will be in a table in the database. That cleaning-up processed will **NOT happen again** as it is now 'persistingly' reflected in the new database. \n",
    "\n",
    "This has the following advantages: \n",
    "- **saves compute time**, especially on large and/or complex datasets\n",
    "- **separates data loading from data consumption**: \n",
    "  - so multiple notebooks can do their own analyses, **consuming** the same database data **without influencing** the data itself.\n",
    "\n",
    "Now, to encapsulate of all of the cleaning-up I did so far and the table/database creation while ensuring that everything is written off and saved safely I am using the context's manager `with` block. I call the table `pedestrian_counts`, which is helb by the database `pedestrian.duckdb`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: Table with name \"pedestrian_counts\" already exists!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatalogException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# context manager with block:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/data_out/pedestrian.duckdb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m      3\u001b[0m     result\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# repeating the cleaning-up steps:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         conn\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate_Time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# copying the whole result into the new table:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m      3\u001b[0m result\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# repeating the cleaning-up steps:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     conn\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate_Time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# copying the whole result into the new table:\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpedestrian_counts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mCatalogException\u001b[0m: Catalog Error: Table with name \"pedestrian_counts\" already exists!"
     ]
    }
   ],
   "source": [
    "# context manager with block:\n",
    "with duckdb.connect(\"../data/data_out/pedestrian.duckdb\") as conn:\n",
    "    # Drop the table if it exists\n",
    "    conn.execute(\"DROP TABLE IF EXISTS pedestrian_counts\")\n",
    "    \n",
    "    result=(\n",
    "        # repeating the cleaning-up steps:\n",
    "        conn.read_csv(\n",
    "            \"../data/data_in/pedestrian_records_2009-2022.csv\",\n",
    "            dtype={\"Date_Time\": \"TIMESTAMP\"},\n",
    "            timestamp_format=\"%B %d, %Y %H:%M:%S %p\",\n",
    "        )\n",
    "        .select(\"* exclude ID\")\n",
    "        .sort(\"Date_Time\")\n",
    "    )\n",
    "    # copying the whole result into the new table:\n",
    "    result.to_table(\"pedestrian_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: \n",
    "\n",
    "I can now start the EDA, however let's first look at some more tools I need: \n",
    "- JupySQl: to conveniently run SQL queries directly within Jupyter cells\n",
    "- Plotly: to create any kind of interactive visualizations, well integrated with Jupyter\n",
    "\n",
    "### SQL directly in Jupyter with JupySQL:\n",
    "\n",
    "To better understand its advantages, I first use the Relational API with the `.sql()` method to:\n",
    "- count the total number of pedestrians \n",
    "- for the _Melbourne Central_ sensor\n",
    "- for 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┐\n",
       "│ Total_Counts │\n",
       "│    int128    │\n",
       "├──────────────┤\n",
       "│      6897406 │\n",
       "└──────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = duckdb.connect(\"../data/data_out/pedestrian.duckdb\")\n",
    "\n",
    "conn.sql(\n",
    "    \"\"\"\n",
    "    select sum(Hourly_Counts) as Total_Counts\n",
    "    from pedestrian_counts\n",
    "    where Year = 2022 and Sensor_Name = 'Melbourne Central'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So obviously the downsides are:\n",
    "- use of a python string (doc strings) to introduce SQL\n",
    "- use of the sql() method too\n",
    "\n",
    "This is similar to what I used in the previous notebook where I used the DuckDB shell directly but within the Jupyter notebook. \n",
    "\n",
    "### **JupySQL Magic SQL**:\n",
    "Here's what JupySQL brings to the table. I am configuring it to be used with our new database, but first I have to close the `conn` one. **REMEMBER**, DuckDB does **NOT allow for multiple concurrent connections** to the same database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Connecting to &#x27;duckdb:///../data/data_out/pedestrian.duckdb&#x27;</span>"
      ],
      "text/plain": [
       "Connecting to 'duckdb:///../data/data_out/pedestrian.duckdb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# enabling SQL Magic\n",
    "%load_ext sql\n",
    "%sql duckdb:///../data/data_out/pedestrian.duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of that, I can now configure it to automatically return a pandas dataframe instead of the usual SQL output, and simplify the output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%config` not found (But line magic `%config` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%config Sqlmagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the lazy evaluation thing? By doing that autopandas configuration I lost that lazy eval. This has performance impacts, for me not su much at it is 'only' a 420MB dataset. If you do that on a very much larger dataset, this will show. \n",
    "\n",
    "And with that I am ready to type SQL magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Total_Counts</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>6897406</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------+\n",
       "| Total_Counts |\n",
       "+--------------+\n",
       "|   6897406    |\n",
       "+--------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select sum(Hourly_Counts) as Total_Counts\n",
    "from pedestrian_counts\n",
    "where Year = 2022 and Sensor_Name = 'Melbourne Central'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right. What about putting all of that in a variable?  \n",
    "\n",
    "### **`%%sql var <<`: to assign a df in a var**:\n",
    "Here's how, by using that syntax: \n",
    "`%%sql var_name <<`\n",
    "\n",
    "I am using it now to:\n",
    "- calculate the total counts\n",
    "- for each sensor \n",
    "- in 2022\n",
    "- and sorts them in descending order (biggest first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql sensors_2022_df <<\n",
    "SELECT Sensor_Name, sum(Hourly_Counts)::BIGINT AS Total_Counts\n",
    "FROM pedestrian_counts\n",
    "WHERE Year = 2022\n",
    "GROUP BY Sensor_Name\n",
    "ORDER BY Total_Counts DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the code above: \n",
    "- ::BIGINT turned the sum of hours count to an integer, removing any decimal that would have appeared otherwise\n",
    "  - Why? Because the sum itself is of data type HUGEINT actually (128-bit integer) preventing the numbers from overflowing (being too big)\n",
    "  - and because pandas does not support 128-bit integers, it converts them to float64 (hence the decimal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sql.run.resultset.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sensors_2022_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I printed the type of the variable `sensors_2022_df` and it is actually not a dataframe.  \n",
    "The `autopandas = True` config did not work as expected.  \n",
    "So I have to do the conversion manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_sensors_2022 = sensors_2022_df.DataFrame()\n",
    "print(type(df_sensors_2022))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, let's look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor_Name</th>\n",
       "      <th>Total_Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flinders La-Swanston St (West)</td>\n",
       "      <td>10492872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Southbank</td>\n",
       "      <td>8737282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melbourne Central</td>\n",
       "      <td>6897406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabeth St - Flinders St (East) - New footpath</td>\n",
       "      <td>6511465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Princes Bridge</td>\n",
       "      <td>6202149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>State Library - New</td>\n",
       "      <td>6049385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flinders Street Station Underpass</td>\n",
       "      <td>5772514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Melbourne Convention Exhibition Centre</td>\n",
       "      <td>5634531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bourke Street Mall (North)</td>\n",
       "      <td>5614610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Melbourne Central-Elizabeth St (East)</td>\n",
       "      <td>5380759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Sensor_Name  Total_Counts\n",
       "0                    Flinders La-Swanston St (West)      10492872\n",
       "1                                         Southbank       8737282\n",
       "2                                 Melbourne Central       6897406\n",
       "3  Elizabeth St - Flinders St (East) - New footpath       6511465\n",
       "4                                    Princes Bridge       6202149\n",
       "5                               State Library - New       6049385\n",
       "6                 Flinders Street Station Underpass       5772514\n",
       "7            Melbourne Convention Exhibition Centre       5634531\n",
       "8                        Bourke Street Mall (North)       5614610\n",
       "9             Melbourne Central-Elizabeth St (East)       5380759"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sensors_2022.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An I can see the top ten sensors ranked by traffic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
