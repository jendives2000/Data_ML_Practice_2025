{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4: Hands-on Exploratory Data Analysis with DuckDB :duck:**\n",
    "\n",
    "---\n",
    "\n",
    "By Jean-Yves Tran | jy.tran@[datascience-jy.com](https://datascience-jy.com) | [LinkedIn](https://www.linkedin.com/in/jytran-datascience/)  \n",
    "IBM Certified Data Analyst \n",
    "\n",
    "---\n",
    "\n",
    "Source: \n",
    "- [Getting Started with DuckDB](https://www.packtpub.com/en-ar/product/getting-started-with-duckdb-9781803232539) by Simon Aubury & Ned Letcher\n",
    "- [DuckDB documentation](https://duckdb.org/docs/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive links in this notebook are not working due to GitHub limitations. View this notebook with the interactive links working [here](https://nbviewer.org/github/jendives2000/Data_ML_Practice_2025/blob/main/1-3-SQL/practice/DuckDB/notebooks/4_duckdb_handson_eda.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part 3 of this series of notebooks on DuckDB.  \n",
    "For an introduction to DuckDB, check [my first notebook](https://github.com/jendives2000/Data_ML_Practice_2025/blob/82571ad44176666f9cf0735c5141c6a96d5eace9/1-3-SQL/practice/DuckDB/notebooks/1_duckdb_intro.ipynb). I also say in there when you should not use DuckDB. \n",
    "For understanding how DuckDB works, check my [second notebook](https://github.com/jendives2000/Data_ML_Practice_2025/blob/ef8533ad82586234cfdc54a494c0c5be590816cc/1-3-SQL/practice/DuckDB/notebooks/2_duckdb_python_API.ipynb).\n",
    "\n",
    "Here I will learn about best practices that:\n",
    "- save time when:\n",
    "  - [querying](#select-) or [inserting](#insert) data to a DuckDB database\n",
    "  - joining tables ([positional](#positional-joins) and [temporal joins](#temporal-joins-asof))\n",
    "\n",
    "<u>**SqlMagic:**</u>  \n",
    "\n",
    "You will notice starting from the insert chapter that I refactored a code that runs SQL commands from the DuckDB shell (CLI). I named it shell_commd(). \n",
    "There is a [better setup](https://duckdb.org/docs/guides/python/jupyter.html) for jupyter-based works that essentially:\n",
    "- runs SQL cells in Jupyter (using just '%sql' before any query) \n",
    "- and delivers the output as a pandas dataframe. \n",
    "\n",
    "Such a setup requires to install and import jupysql, pandas of course and optionally matplotlib and duck-engine.  \n",
    "I am **not using this setup** in this notebook but in the next one, which will be focused on data exploration and visualization. \n",
    "\n",
    "**OUTLINE:**  \n",
    "In more details, I will cover the followings:\n",
    "- [**Selecting columns**](#select-) effectively\n",
    "- Applying [**function chaining**](#function-chaining)\n",
    "- Using [**INSERT**](#insert) effectively\n",
    "- Leveraging [**positional** joins](#positional-joins) and [**temporal joins**](#temporal-joins-asof)\n",
    "- [**Recursive** queries](#hierarchical-traversal) and [macros](#macros)\n",
    "- additional **tips and tricks**\n",
    "\n",
    "**NICETIES:**  \n",
    "Some of the nicest little improvements brought by DuckDB are: \n",
    "- [trailing comma](#trailing-comma-is-fine) not a problem\n",
    "- the [exclude](#exclude-columns) clause\n",
    "- [visual bars](#visualize-bars-in-the-output) in the output\n",
    "- [insert or replace](#insert-or-replace)\n",
    "\n",
    "**SKIERS DATABASE**:  \n",
    "I added several very simple Skiers Database and csv files in the data/data_in folder: `skiers.csv`  \n",
    "They'll be used throughout this notebook. \n",
    "\n",
    "**The main takeaway is**:\n",
    "- to **better comprehend** the **differences** between regular SQL queries and DuckDB enhanced queries, which is often a lot less verbosity and more readability. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas matplotlib\n",
    "import duckdb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Melbourne Pedestrian Count:\n",
    "\n",
    "I will be using a dataset made available by the city of Melbourne, Australia. contains hourly pedestrian counts from pedestrian sensors located in and around the Melbourne Central business district. We’ll be working with a historical timeframe of this dataset ranging from 2009 to 2022.\n",
    "\n",
    "I [imported](https://data.melbourne.vic.gov.au/api/datasets/1.0/pedestrian-counting-system-monthly-counts-per-hour/attachments/pedestrian_counting_system_monthly_counts_per_hour_may_2009_to_14_dec_2022_csv_zip/) it in the data/data_in folder. \n",
    "Once unzipped, this is:\n",
    "- a 420MB dataset \n",
    "- with over 2,1 million entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational API: \n",
    "\n",
    "I will use the Relational API because it offers more for what I will be doing: data exploratory analysis. \n",
    "\n",
    "Let's get our dataset into a Relational Object (RO from now on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬───────────────────────────────┬───────┬──────────┬───────┬──────────┬───────┬───────────┬───────────────────────────────┬───────────────┐\n",
      "│   ID    │           Date_Time           │ Year  │  Month   │ Mdate │   Day    │ Time  │ Sensor_ID │          Sensor_Name          │ Hourly_Counts │\n",
      "│  int64  │            varchar            │ int64 │ varchar  │ int64 │ varchar  │ int64 │   int64   │            varchar            │     int64     │\n",
      "├─────────┼───────────────────────────────┼───────┼──────────┼───────┼──────────┼───────┼───────────┼───────────────────────────────┼───────────────┤\n",
      "│ 2887628 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        34 │ Flinders St-Spark La          │           300 │\n",
      "│ 2887629 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        39 │ Alfred Place                  │           604 │\n",
      "│ 2887630 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        37 │ Lygon St (East)               │           216 │\n",
      "│ 2887631 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        40 │ Lonsdale St-Spring St (West)  │           627 │\n",
      "│ 2887632 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        36 │ Queen St (West)               │           774 │\n",
      "│ 2887633 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        29 │ St Kilda Rd-Alexandra Gardens │           644 │\n",
      "│ 2887634 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        42 │ Grattan St-Swanston St (West) │           453 │\n",
      "│ 2887635 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        43 │ Monash Rd-Swanston St (West)  │           387 │\n",
      "│ 2887636 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        44 │ Tin Alley-Swanston St (West)  │            27 │\n",
      "│ 2887637 │ November 01, 2019 05:00:00 PM │  2019 │ November │     1 │ Friday   │    17 │        35 │ Southbank                     │          2691 │\n",
      "│    ·    │               ·               │    ·  │    ·     │     · │   ·      │     · │         · │     ·                         │            ·  │\n",
      "│    ·    │               ·               │    ·  │    ·     │     · │   ·      │     · │         · │     ·                         │            ·  │\n",
      "│    ·    │               ·               │    ·  │    ·     │     · │   ·      │     · │         · │     ·                         │            ·  │\n",
      "│ 2897597 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        27 │ QV Market-Peel St             │           371 │\n",
      "│ 2897598 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        28 │ The Arts Centre               │          1188 │\n",
      "│ 2897599 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        31 │ Lygon St (West)               │           229 │\n",
      "│ 2897600 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        30 │ Lonsdale St (South)           │           391 │\n",
      "│ 2897601 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        34 │ Flinders St-Spark La          │           111 │\n",
      "│ 2897602 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        37 │ Lygon St (East)               │           133 │\n",
      "│ 2897603 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        40 │ Lonsdale St-Spring St (West)  │           154 │\n",
      "│ 2897604 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        36 │ Queen St (West)               │           249 │\n",
      "│ 2897605 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        29 │ St Kilda Rd-Alexandra Gardens │           448 │\n",
      "│ 2897606 │ November 09, 2019 10:00:00 AM │  2019 │ November │     9 │ Saturday │    10 │        42 │ Grattan St-Swanston St (West) │           288 │\n",
      "├─────────┴───────────────────────────────┴───────┴──────────┴───────┴──────────┴───────┴───────────┴───────────────────────────────┴───────────────┤\n",
      "│ ? rows (>9999 rows, 20 shown)                                                                                                          10 columns │\n",
      "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = duckdb.read_csv(\"../data/data_in/pedestrian_records_2009-2022.csv\")\n",
    "\n",
    "# 200 is the max number of characters possible inside an entry:\n",
    "records.show(max_width = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looking at the Data**:\n",
    "\n",
    "Because this is an RO, DuckDB loaded 10,000 rows and this what it **lazily returned** us here. There more entries than these 10,000. \n",
    "\n",
    "Lots of info in this dataset: \n",
    "- **count of pedestrians** detected by \n",
    "- a specific **sensor** \n",
    "- during **each hour**. \n",
    "- Additionally, it provides other details related to the hourly readings, including the **sensor name** \n",
    "  - and the **timestamp**, along with date and time components derived from the timestamp.\n",
    "\n",
    "### **Data types**:\n",
    "Notice in the header of the output that datetime is of the data type VARCHAR, meaning text. It should be a timestamp (docs on this [here](http://duckdb.org/docs/sql/functions/dateformat)). Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = duckdb.read_csv(\n",
    "    \"../data/data_in/pedestrian_records_2009-2022.csv\",\n",
    "    dtype={\"Date_Time\": \"TIMESTAMP\"},\n",
    "    timestamp_format=\"%B %d, %Y %H:%M:%S %p\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm this change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬─────────────────────┬───────┬──────────┬───────┬─────────┬───────┬───────────┬──────────────────────────────┬───────────────┐\n",
      "│   ID    │      Date_Time      │ Year  │  Month   │ Mdate │   Day   │ Time  │ Sensor_ID │         Sensor_Name          │ Hourly_Counts │\n",
      "│  int64  │      timestamp      │ int64 │ varchar  │ int64 │ varchar │ int64 │   int64   │           varchar            │     int64     │\n",
      "├─────────┼─────────────────────┼───────┼──────────┼───────┼─────────┼───────┼───────────┼──────────────────────────────┼───────────────┤\n",
      "│ 2887628 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        34 │ Flinders St-Spark La         │           300 │\n",
      "│ 2887629 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        39 │ Alfred Place                 │           604 │\n",
      "│ 2887630 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        37 │ Lygon St (East)              │           216 │\n",
      "│ 2887631 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        40 │ Lonsdale St-Spring St (West) │           627 │\n",
      "│ 2887632 │ 2019-11-01 17:00:00 │  2019 │ November │     1 │ Friday  │    17 │        36 │ Queen St (West)              │           774 │\n",
      "└─────────┴─────────────────────┴───────┴──────────┴───────┴─────────┴───────┴───────────┴──────────────────────────────┴───────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records.limit(5).show(max_width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enums for low cardinality String Columns**:\n",
    "\n",
    "[ENUM types](https://duckdb.org/docs/sql/data_types/enum.html) are a way to **convert string values into numbers** in a database. This is useful for columns with a limited number of different values, like month names and days of the week. By using ENUMs for these columns, we can **save storage space** and **speed up queries** because the database only stores numbers instead of full strings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DuckDB, because the **SQL parser doesn’t support subqueries** in a CREATE TYPE ... AS ENUM statement, the best approach is to **extract** the distinct values via SQL (or pandas), **format** them into a comma-separated list, and then **build** the ENUM type dynamically.  \n",
    "This method gives me flexibility and control over the ENUM values while keeping my code concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactored logic to create ENUM with an RO:\n",
    "def create_enum(col, db, enum_name):\n",
    "    \"\"\"\n",
    "    Create an ENUM type in DuckDB from unique values in a specified column.\n",
    "\n",
    "    Parameters:\n",
    "    col (str): The column name to extract unique values from.\n",
    "    db (str): The name of the DuckDB relation (table).\n",
    "    enum_name (str): The name of the ENUM type to be created.\n",
    "    \"\"\"\n",
    "    # extracting the unique values from the specified column\n",
    "    col_forEnum = duckdb.sql(\n",
    "        f\"\"\"\n",
    "        select distinct {col}\n",
    "        from {db}\n",
    "        \"\"\"\n",
    "    ).fetchall()\n",
    "\n",
    "    # Build a comma-separated list of ENUM values\n",
    "    col_enum_val = \", \".join(f\"'{v[0]}'\" for v in col_forEnum)\n",
    "\n",
    "    # create the ENUM type\n",
    "    duckdb.sql(f\"create type {enum_name} as enum ({col_enum_val});\")\n",
    "\n",
    "create_enum(\"Month\", \"records\", \"month_enum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_enum(\"Day\", \"records\", \"day_enum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did I actually created them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                enum_range(CAST(NULL AS month_enum))                                │\n",
      "│                                             varchar[]                                              │\n",
      "├────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ [October, April, July, January, March, August, September, December, May, June, February, November] │\n",
      "└────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────┐\n",
      "│                enum_range(CAST(NULL AS day_enum))                │\n",
      "│                            varchar[]                             │\n",
      "├──────────────────────────────────────────────────────────────────┤\n",
      "│ [Thursday, Wednesday, Monday, Friday, Saturday, Tuesday, Sunday] │\n",
      "└──────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(duckdb.sql(\"select enum_range(NULL::month_enum);\"))\n",
    "print(duckdb.sql(\"select enum_range(NULL::day_enum);\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we load our dataset into an on-disk database for analysis, we should think about any data changes we might want to make. For instance, we can remove columns we won’t need. The additional date and time fields are likely to be useful, and we should keep the Sensor_ID column since it helps connect this dataset with another one from the city of Melbourne that includes details about each sensor, like their locations. However, we can drop the ID field because it doesn’t relate to any other datasets about the pedestrian counting system. Since we’ll be doing various time series analyses, we also need to sort the records by the Date_Time column. Let’s make these changes and take a look at the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
