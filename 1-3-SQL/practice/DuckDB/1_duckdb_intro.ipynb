{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Intro to DuckDB\n",
    "\n",
    "---\n",
    "\n",
    "By Jean-Yves Tran | jy.tran@[datascience-jy.com](https://datascience-jy.com) | [LinkedIn](https://www.linkedin.com/in/jytran-datascience/)  \n",
    "IBM Certified Data Analyst \n",
    "\n",
    "---\n",
    "\n",
    "Source: \n",
    "- [Getting Started with DuckDB](https://www.packtpub.com/en-ar/product/getting-started-with-duckdb-9781803232539) by Simon Aubury & Ned Letcher\n",
    "- [DuckDB documentation](https://duckdb.org/docs/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive links in this notebook are not working due to GitHub limitations. View this notebook with the interactive links working [here](https://nbviewer.org/github/jendives2000/Data_ML_Practice_2025/blob/main/1-3-SQL/practice/dvd_rental/3_PostGreSQL_SQLAlchemy_Biz-database_dvdrental_3.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an introduction on DuckDB, an increasingly popular (2025) SQL toolkit. I will quickly introduce the what and the why so that I can show some SQL queries as soon as possible. \n",
    "\n",
    "## **SQLite for OLAP = DuckDB**\n",
    "DuckDB is indeed similar to SQLite in that both are **in-process** databases that **write to a single-file** storage format and are **free and open source**.  \n",
    "\n",
    "**Key difference**:  \n",
    "SQLite is optimized for transactional **(OLTP) workloads** (it is very good at reading rows, not columns), while DuckDB is designed specifically for analytical queries because of its **optimization for OLAP workloads** (columns).\n",
    "Often referred to as ‚ÄúSQLite for OLAP‚Äù, as it is the **first production-ready in-process OLAP DBMS**.\n",
    "\n",
    "### **In-process**: \n",
    "Being in-process means that DuckDB runs **inside the same process as the application**, eliminating the need for a separate database service. This makes it lightweight, fast, and easy to integrate, especially for analytical **workloads where data is processed locally** rather than stored on a central database server.\n",
    "\n",
    "**No Client-Server Communication**\n",
    "\n",
    "Traditional databases like PostgreSQL or MySQL run as separate services (often on a remote server) and require network communication between the application and the database.\n",
    "DuckDB, being in-process, does **not require network communication** because it runs directly within the application‚Äôs memory space. This makes DuckDB ideal for fast, interactive analytics on local data.\n",
    "\n",
    "DuckDB is an **embedded, high-performance OLAP database designed for analytics, offering SQL-based querying and a simplified installation process**, making it an attractive alternative for analytical workloads compared to traditional client-server databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why Use DuckDB**?\n",
    "1. Two Main Use Cases\n",
    "- **Analytical Workflows** ‚Üí For data analysts, data scientists, and ML engineers, DuckDB enables fast, **scalable**, and SQL-based **data wrangling, exploration, and transformation**.  \n",
    "  \n",
    "- **Operational Data Infrastructure & Interactive Apps** ‚Üí For data engineers and software developers, DuckDB can **power ETL** pipelines, **lightweight data lakes**, and **real-time dashboards** with low-latency analytics.\n",
    "\n",
    "### **Use case 1: Analytical Workflows?**\n",
    "- Handles **large datasets efficiently** on a single machine, **reducing the need** for complex distributed systems like Spark.\n",
    "- Works out-of-core, meaning it can **process datasets larger than memory**.\n",
    "- **Faster** than Pandas, Dask, and Polars **for complex** SQL-based analytics.\n",
    "- Provides **DBMS features** (ACID transactions, constraints, data integrity) that **dataframe libraries lack**.\n",
    "\n",
    "### **Enhanced Performance**\n",
    "‚ö° **Optimized for OLAP** ‚Üí Efficient for columnar analytics, large joins, and aggregations\n",
    "üîó Seamless integration with **CSV, Parquet, JSON**, and external databases like **PostgreSQL & MySQL**\n",
    "üîÑ Works with **Pandas, Polars, R, and Arrow** ‚Üí Bridges the gap between databases and dataframe workflows\n",
    "\n",
    "### **Use case 2: Building Block for Data Infrastructure**\n",
    "üîπ **Can replace Apache Spark in some ETL workflows** by running transformations locally\n",
    "üîπ A **lightweight alternative to Google BigQuery, Snowflake, and ClickHouse** for medium-scale data warehouses\n",
    "üîπ Used for **real-time dashboards & BI tools** (e.g., Mode, Hex, Rill) that need fast query performance.\n",
    "\n",
    "### **DuckDB as a Pre-Scaling SQL Solution**\n",
    "DuckDB is highly valuable for companies **until they truly need to scale out** to a cloud-based distributed system. It serves as an efficient, high-performance SQL engine for managing and analyzing data locally before cloud-scale infrastructure becomes necessary.\n",
    "\n",
    "**When to Scale Beyond DuckDB?**\n",
    "üöÄ You need multi-user, concurrent access.\n",
    "üöÄ Your data size exceeds what can be efficiently processed on a single machine.\n",
    "üöÄ Your company needs cloud-native, real-time distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Versatility & Other Niceties**: \n",
    "\n",
    "Not only can DuckDB support Parquet, CSV, and JSON formats and read/write files from disk or cloud storage (S3). It can also **connect to PostgreSQL, MySQL, and SQLite** to run queries across multiple databases.\n",
    "\n",
    "Here are other nice things DuckDB offers (among other things):  \n",
    "- **Efficient Storage** ‚Üí DuckDB uses compression algorithms to reduce disk storage, improving read performance.\n",
    "- **Query Optimization** ‚Üí DuckDB rewrites queries for better efficiency instead of running them as written.\n",
    "- **Parallel Processing** ‚Üí Most operations automatically run on multiple CPU threads, speeding up queries.\n",
    "- **Python-Like Syntax** ‚Üí Supports simple string and list slicing, list comprehensions, and lambda functions in SQL.\n",
    "- **Chained Function Calls** ‚Üí Similar to pandas, allowing method-style query chaining.\n",
    "- **Trailing Commas** ‚Üí SQL queries support trailing commas for cleaner syntax.\n",
    "\n",
    "### **Install and use it almost everywhere**: \n",
    "- **Multiple Client APIs**‚Üí DuckDB supports Python, R, JavaScript, Rust, Swift, Julia, Java, C, and C++ for easy integration.\n",
    "- **JavaScript Support** ‚Üí Two versions: Node.js (backend use) and Wasm (runs in a web browser).\n",
    "- **Command-Line Interface (CLI)** ‚Üí A cross-platform executable that works on any system with a command line.\n",
    "- **Community-Supported Clients** ‚Üí Additional support for Go, C#, Ruby, Common Lisp, Zig, and Crystal.\n",
    "- **Flexible Deployment** ‚Üí Can be used in analytical workflows, operational infrastructure, and data products across diverse environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **When not to use DuckDB**:\n",
    "- **Not for OLTP** ‚Üí DuckDB is built for analytical (OLAP) workloads, not transactional (OLTP) use cases with frequent small writes. If you need an in-process OLTP database, SQLite is a better choice.\n",
    "- **Single Process Write Limitation** ‚Üí Only one process can write to a DuckDB database at a time. Multiple processes can read, but only in read-only mode.\n",
    "- **Not for Petabyte-Scale Data** ‚Üí While DuckDB is fast on a single machine, it‚Äôs not designed for distributed computing. If you need to process petabyte-scale data, a cloud-based or distributed system is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing it:  \n",
    "\n",
    "Very straightforward: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install duckdb --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now need to activate it in the bash with this simple command:  \n",
    "`duckdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, cast, alias, select, and_, extract, union, literal, desc, nullsfirst, func, Time, Table, MetaData, Date, text, Integer, case, Column, Text, Numeric, true\n",
    "from sqlalchemy.types import String\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from shared_code import execute_stmt, subquery, sql_2_df\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
